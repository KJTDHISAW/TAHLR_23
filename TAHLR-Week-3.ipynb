{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLTK Book: Ch. 3. Processing raw text\n\nCode-only version of Chapter 3 of the [NLTK Book](https://www.nltk.org/book/ch03.html) for use in the TAHLR course","metadata":{}},{"cell_type":"markdown","source":"## 1 Accessing Text Corpora\n\n> The goal of this chapter is to answer the following questions:\n> - How can we write programs to access text from local files and from the web, in order to get hold of an unlimited range of language material?\n> - How can we split documents up into individual words and punctuation symbols, so we can carry out the same kinds of analysis we did with text corpora in earlier chapters?\n> -How can we write programs to produce formatted output and save it in a file?\n","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Accessing text from the web and from disk","metadata":{}},{"cell_type":"code","source":"import nltk\nimport re\nfrom pprint import pprint\nfrom nltk import word_tokenize","metadata":{"trusted":true,"tags":[]},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from urllib import request\n\nurl = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\nresponse = request.urlopen(url)\nraw = response.read().decode('utf-8-sig').strip()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(raw)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(raw)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw[:75]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenization\n\ntokens = word_tokenize(raw)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(tokens)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tokens)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokens[:10])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = nltk.Text(tokens)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(text[1075:1113])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text.collocations()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw.find(\"PART I\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw.rfind(\"*** END OF THE PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = raw.find(\"PART I\")\nend = raw.rfind(\"*** END OF THE PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\")\nraw_ = raw[start:end]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(raw_[:100])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(raw_[-100:])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dealing with HTML, Processing Search-Engine Results, Processing RSS Feeds\n\n# Consult book","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading local files\n# https://sa.wikisource.org/wiki/%E0%A4%8B%E0%A4%97%E0%A5%8D%E0%A4%B5%E0%A5%87%E0%A4%A6%E0%A4%83_%E0%A4%B8%E0%A5%82%E0%A4%95%E0%A5%8D%E0%A4%A4%E0%A4%82_%E0%A5%A7%E0%A5%A6.%E0%A5%A7%E0%A5%A8%E0%A5%AF\n\nf = open('Plin_33.txt')\np33_raw = f.read()\nf.close()","metadata":{"trusted":true,"tags":[]},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Preferred method\n\nwith open('Plin_33.txt') as f:\n    p33_raw = f.read()","metadata":{"trusted":true,"tags":[]},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"p33_raw.split('\\n')[:20]","metadata":{"trusted":true,"tags":[]},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['Liber XXXIII',\n '   ',\n 'i\\t  ',\n '1\\t',\n 'Metalla nunc ipsaeque opes et rerum pretia dicentur, tellurem intus exquirente cura multiplici modo, quippe alibi divitiis foditur quaerente vita aurum, argentum, electrum, aes, alibi deliciis gemmas et parietum lignorumque pigmenta, alibi temeritati ferrum, auro etiam gratius inter bella caedesque. persequimur omnes eius fibras vivimusque super excavatam, mirantes dehiscere aliquando aut intremescere illam, ceu vero non hoc indignatione sacrae parentis exprimi possit.',\n '2\\t',\n 'imus in viscera et in sede manium opes quaerimus, tamquam parum benigna fertilique qua calcatur. et inter haec minimum remediorum gratia scrutamur, quoto enim cuique fodiendi causa medicina est? quamquam et hoc summa sui parte tribuit ut fruges, larga facilisque in omnibus, quaecumque prosunt.',\n '',\n '3\\t',\n 'illa nos peremunt, illa nos ad inferos agunt, quae occultavit atque demersit, illa, quae non nascuntur repente, ut mens ad inane evolans reputet, quae deinde futura sit finis omnibus saeculis exhauriendi eam, quo usque penetratura avaritia. quam innocens, quam beata, immo vero etiam delicata esset vita, si nihil aliunde quam supra terras concupisceret, breviterque, nisi quod secum est!',\n '   ',\n 'ii\\t  ',\n '4\\t',\n 'Eruitur aurum et chrysocolla iuxta, ut pretiosior videatur, nomen ex auro custodiens. parum enim erat unam vitae invenisse pestem, nisi in pretio esset auri etiam sanies. quaerebat argentum avaritia; boni consuluit interim invenisse minium rubentisque terrae excogitavit usum. heu prodigia ingenia, quot modis auximus pretia rerum! accessit ars picturae, et aurum argentumque caelando carius fecimus. didicit homo naturam provocare. auxere et artem vitiorum inritamenta; in poculis libidines caelare iuvit ac per obscenitates bibere.',\n '',\n '5\\t',\n 'abiecta deinde sunt haec ac sordere coepere, ut auri argentique nimium fuit. murrina ex eadem tellure et crystallina effodimus, quibus pretium faceret ipsa fragilitas. hoc argumentum opum, haec vera luxuriae gloria existimata est, habere quod posset statim perire totum. nec hoc fuit satis. turba gemmarum potamus et zmaragdis teximus calices, ac temulentiae causa tenere Indiam iuvat. aurum iam accessio est,',\n '  ',\n 'iii\\t6\\t',\n 'utinamque posset e vita in totum abdicari [sacrum fame, ut celeberrimi auctores dixere] proscissum conviciis ab optimis quibusque et ad perniciem vitae repertum, quanto feliciore aevo, cum res ipsae permutabantur inter sese, sicut et Troianis temporibus factitatum Homero credi convenit! ita enim, ut opinor, commercia victus gratia invecta.']"},"metadata":{}}]},{"cell_type":"code","source":"import os\nos.listdir('../data/texts/')","metadata":{"trusted":true,"tags":[]},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['lyoc', 'document.txt', 'output']"},"metadata":{}}]},{"cell_type":"code","source":"[file for file in os.listdir('../data/texts/') if file.endswith('.txt')]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\n\nprint(glob('../data/texts/*.txt'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../data/texts/document.txt', 'r') as f:\n    for line in f:\n        print(line.strip())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The NLP Pipeline","metadata":{}},{"cell_type":"code","source":"tokens = word_tokenize(p33_raw)\nprint(tokens[:20])","metadata":{"trusted":true,"tags":[]},"execution_count":13,"outputs":[{"name":"stdout","text":"['Liber', 'XXXIII', 'i', '1', 'Metalla', 'nunc', 'ipsaeque', 'opes', 'et', 'rerum', 'pretia', 'dicentur', ',', 'tellurem', 'intus', 'exquirente', 'cura', 'multiplici', 'modo', ',']\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab = sorted(set(tokens))\nprint(vocab[:20])","metadata":{"trusted":true,"tags":[]},"execution_count":15,"outputs":[{"name":"stdout","text":"['!', '(', ')', ',', '.', '1', '10', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '11', '110', '111']\n","output_type":"stream"}]},{"cell_type":"code","source":"raw = 'It was the best of times. It was the worst of times.'\ntokens = word_tokenize(raw)\nprint(tokens)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = sorted(set(tokens))\nprint(vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 3.2 Strings: Text Processing at the Lowest Level","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monty = 'Monty Python'\ncircus = \"Monty Python's Flying Circus\"\ncircus = 'Monty Python\\'s Flying Circus'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"couplet = \"Shall I compare thee to a Summer's day?\"\\\n            \"Thou are more lovely and more temperate:\"\nprint(couplet)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"couplet = (\"Rough winds do shake the darling buds of May,\"\n            \"And Summer's lease hath all too short a date:\")\nprint(couplet)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\"\"Shall I compare thee to a Summer's day?\nThou are more lovely and more temperate:\"\"\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'very' + 'very' + 'very'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'very' * 3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''.join(['very', 'very', 'very'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"' '.join(['very', 'very', 'very'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monty","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(monty)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monty[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monty[3]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monty[5]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# monty[20]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monty[-1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monty[len(monty) - 7]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monty[-7]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent = 'colorless green ideas sleep furiously'\nfor char in sent:\n    print(char, end=' ')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import gutenberg\nraw = gutenberg.raw('melville-moby_dick.txt')\nfdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fdist.most_common(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([char for (char, count) in fdist.most_common()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len([char for (char, count) in fdist.most_common()]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Monty Python\n#  012345678901\n#- 210987654321\n\nmonty[6:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monty[-12:-7]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monty[:5]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monty[6:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phrase = 'And now for something completely different'\nif 'thing' in phrase:\n    print('found \"thing\"')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not 'nothing' in phrase:\n    print('found nothing')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" monty.find('Python')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Regular expressions for detecting word patterns","metadata":{}},{"cell_type":"code","source":"wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# $ = end of string\nprint([w for w in wordlist if re.search('ed$', w)][:5])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ^ = start of string\n# $ = end of string\n# . = any character\n\nprint([w for w in wordlist if re.search('^..j..t..$', w)][:5])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([w for w in chat_words if re.search('^m+i+n+e+$', w)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nhaha = [w for w in chat_words if re.search('^[ha]+$', w)]\nhaha_sample = random.sample(haha, 5)\nprint(haha_sample)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wsj = sorted(set(nltk.corpus.treebank.words()))\nprint(random.sample([w for w in wsj if re.search('^[0-9]{4}$', w)],5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(random.sample([w for w in wsj if re.search('(ed|ing)$', w)],5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5 Useful Applications of Regular Expressions","metadata":{}},{"cell_type":"code","source":"word = 'supercalifragilisticexpialidocious'\nprint(re.findall(r'[aeiou]', word))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(re.findall(r'[aeiou]', word))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wsj = sorted(set(nltk.corpus.treebank.words()))\nfd = nltk.FreqDist(vs for word in wsj\n                    for vs in re.findall(r'[aeiou]{2,}', word))\npprint(fd.most_common(12))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.6 Normalizing text","metadata":{}},{"cell_type":"code","source":"raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government.  Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\ntokens = word_tokenize(raw)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"porter = nltk.PorterStemmer()\nlancaster = nltk.LancasterStemmer()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([porter.stem(t) for t in tokens])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([lancaster.stem(t) for t in tokens])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lemmatization\n\nwnl = nltk.WordNetLemmatizer()\nprint([wnl.lemmatize(t) for t in tokens])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.7. Regular expressions for tokenizing text\n\n\"Tokenization turns out to be a far more difficult task than you might have expected.\"","metadata":{}},{"cell_type":"code","source":"raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\nthough), 'I won't have any pepper in my kitchen AT ALL. Soup does very\nwell without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(re.split(r' ', raw))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cf. \nprint(raw.split(' '))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\nsents = nltk.sent_tokenize(text)\nfor i, sent in enumerate(sents[79:89]):\n    print(f'{i}: {sent}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rv_raw)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"units = re.split(r'[редрее]\\n', rv_raw)\nprint(units[:4])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Formatting: From lists to strings","metadata":{}},{"cell_type":"code","source":"silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"' '.join(silly)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'|'.join(silly)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''.join(silly)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# String formatting\n\nfdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n\nfor word in sorted(fdist):\n    print(f'{word} -> {fdist[word]}', end='; ')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nf'{math.pi:.4f}'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count, total = 3205, 9375\nf'accuracy for {total} words: {100 * count/total:.4f}%'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install tabulate","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tabulate import tabulate\n\nfrom nltk.corpus import brown\ncfd = nltk.ConditionalFreqDist(\n    (genre, word)\n    for genre in brown.categories()\n    for word in brown.words(categories=genre))\n\ngenres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\nmodals = ['can', 'could', 'may', 'might', 'must', 'will']\n\nprint(tabulate(cfd.tabulate(conditions=genres, samples=modals)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../data/texts/output/kjv-words.txt', 'w') as f:\n    words = set(nltk.corpus.genesis.words('english-kjv.txt'))\n    for word in sorted(words, key=str.lower):\n        if word.isalpha():\n            f.write(f'{word}\\n')\n","metadata":{},"execution_count":null,"outputs":[]}]}